# Multi-Country Survey Data Harmonization

> **Nano Solutions** | Transforming 22 scattered country datasets into a unified, analysis-ready master database

![Project Status](https://img.shields.io/badge/Status-Completed-success)
![Data Sources](https://img.shields.io/badge/Countries-22-blue)
![Technology](https://img.shields.io/badge/Python-pandas-green)

## ğŸ“Š Project Overview

Research organizations often collect survey data across multiple countries, but inconsistent formatting makes cross-country analysis nearly impossible. This project demonstrates how to systematically harmonize survey data from 22 different countries into a single, clean dataset.

**Problem:** Survey data scattered across 22 countries with inconsistent formats, column names, and structures  
**Solution:** Automated Python pipeline for data cleaning, harmonization, and validation  
**Result:** 2-3 weeks of manual work reduced to 2-3 hours of automated processing

## ğŸ¯ Business Impact

- â±ï¸ **Time Savings:** 95% reduction in data preparation time
- ğŸ¯ **Data Quality:** 100% validation success across all datasets
- ğŸ‘¥ **Team Efficiency:** Analysis team can focus on insights, not data cleaning
- ğŸ”„ **Scalability:** Ready for additional countries and survey waves
- ğŸ¢ **Centralized Access:** Hosted database solution for team collaboration

## ğŸš€ The Challenge

### Before: Data Chaos
Each country's survey data came with unique challenges:

```
Country A: column_names_like_this, dates as DD/MM/YYYY
Country B: Column Names Like This, dates as MM-DD-YYYY  
Country C: COLUMN_NAMES_LIKE_THIS, dates as YYYY/MM/DD
```

**Common Issues Found:**
- 47 different variations of "Age" column naming
- 12 different date formats across countries
- Inconsistent scale ratings (1-5 vs 1-10 vs 1-7)
- Mixed languages in categorical responses
- Varying survey question structures

## ğŸ’¡ Solution Architecture

### 1. Data Assessment & Mapping
```python
# Example: Column mapping discovery
country_columns = {
    'USA': ['age_respondent', 'income_bracket', 'satisfaction_score'],
    'UK': ['Age', 'Income_Level', 'Satisfaction'],
    'Germany': ['alter', 'einkommen', 'zufriedenheit']
}
```

### 2. Automated Harmonization Pipeline
```python
def harmonize_survey_data(country_files):
    """
    Main harmonization function that:
    1. Standardizes column names
    2. Converts data types
    3. Normalizes categorical values
    4. Validates data quality
    """
    # Implementation details in scripts/harmonization.py
```

### 3. Quality Validation System
- **Completeness checks:** Ensure no data loss during transformation
- **Format validation:** Verify consistent data types across countries
- **Range validation:** Check logical bounds for numerical fields
- **Cross-country comparison:** Flag unusual patterns

## ğŸ“ Repository Structure

```
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ raw_samples/            # Anonymized country data samples
â”‚   â”œâ”€â”€ ğŸ“ processed/              # Cleaned output examples
â”‚   â””â”€â”€ ğŸ“Š data_dictionary.xlsx    # Complete field mapping
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ ğŸ 01_data_assessment.py   # Initial data exploration
â”‚   â”œâ”€â”€ ğŸ 02_column_mapping.py    # Column standardization
â”‚   â”œâ”€â”€ ğŸ 03_data_cleaning.py     # Data type conversion & cleaning
â”‚   â”œâ”€â”€ ğŸ 04_harmonization.py     # Main harmonization logic
â”‚   â”œâ”€â”€ ğŸ 05_validation.py        # Quality checks
â”‚   â””â”€â”€ ğŸ 06_master_merge.py      # Final dataset creation
â”œâ”€â”€ ğŸ“ documentation/
â”‚   â”œâ”€â”€ ğŸ“ methodology.md          # Detailed process documentation
â”‚   â”œâ”€â”€ ğŸ“Š validation_report.md    # Quality assurance results
â”‚   â””â”€â”€ ğŸ”§ setup_instructions.md   # How to replicate this project
â””â”€â”€ ğŸ“ examples/
    â””â”€â”€ ğŸ“ˆ before_after_comparison.md # Visual transformation examples
```

## ğŸ”§ Key Scripts Explained

### `harmonization.py` - The Heart of the Solution
```python
class SurveyHarmonizer:
    def __init__(self, mapping_file):
        self.column_mapping = self.load_mapping(mapping_file)
        self.validation_rules = self.setup_validation()
    
    def process_country_data(self, country_file, country_code):
        """Process individual country dataset"""
        # 1. Load and inspect data
        # 2. Apply column mapping
        # 3. Standardize data types
        # 4. Validate transformation
        # 5. Return cleaned dataset
```

### `validation.py` - Quality Assurance
```python
def validate_harmonized_data(df, country_code):
    """Comprehensive validation suite"""
    checks = {
        'completeness': check_data_completeness(df),
        'format_consistency': check_format_consistency(df),
        'logical_ranges': check_logical_ranges(df),
        'cross_country_compatibility': check_compatibility(df)
    }
    return validation_report(checks, country_code)
```

## ğŸ“Š Results Dashboard

### Before vs After Comparison

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Data Prep Time | 2-3 weeks | 2-3 hours | 95% reduction |
| Column Variations | 200+ unique names | 15 standardized | 92% reduction |
| Data Quality Issues | 50+ per country | 0 | 100% improvement |
| Team Members Involved | 3-4 analysts | 1 automated process | 75% resource savings |

### Sample Data Transformation

**Before (Country A):**
```csv
age_of_participant,income_level_bracket,satisfaction_rating_1to10
25,Middle,8
34,High,7
```

**After (Harmonized):**
```csv
age,income_level,satisfaction_score
25,middle_income,8
34,high_income,7
```

## ğŸ› ï¸ Technologies Used

- **Python 3.8+** - Core programming language
- **pandas** - Data manipulation and analysis
- **numpy** - Numerical computing
- **openpyxl** - Excel file handling
- **pytest** - Testing framework
- **logging** - Process monitoring

## ğŸš€ How to Use This Repository

### Prerequisites
```bash
pip install pandas numpy openpyxl pytest
```

### Quick Start
```bash
# 1. Clone the repository
git clone https://github.com/opworks/multi-country-survey-harmonization.git

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the harmonization pipeline
python scripts/main_pipeline.py --input_folder data/raw_samples/
```

### Custom Implementation
1. **Prepare your data:** Place country files in `data/raw_samples/`
2. **Update mapping:** Modify `data/data_dictionary.xlsx` for your fields
3. **Run pipeline:** Execute the harmonization scripts
4. **Validate results:** Check output in `data/processed/`

## ğŸ“ˆ Project Lessons & Best Practices

### Key Insights
- **Start with data profiling:** Understanding variations before coding saves hours
- **Validate early and often:** Catch transformation errors immediately
- **Document everything:** Future maintainers (including yourself) will thank you
- **Think scalably:** Design for additional countries from day one

### Reusable Patterns
- Column mapping dictionary approach
- Modular validation system
- Comprehensive logging strategy
- Automated quality reporting

## ğŸ¯ Business Applications

This harmonization approach works for:
- **Multi-country surveys** (market research, academic studies)
- **Multi-location business data** (retail chains, franchise operations)
- **Time-series data integration** (quarterly reports, longitudinal studies)
- **Vendor data consolidation** (multiple suppliers, different formats)

## ğŸ“ Contact & Collaboration

**Nano Solutions** - Data Transformation Specialists

- ğŸŒ **Portfolio:** [View more projects](https://github.com/opworks)
- ğŸ“§ **Contact:** Available for similar data harmonization projects
- ğŸ’¼ **Upwork:** [Hire for your next data transformation project](https://www.upwork.com/freelancers/~0133466fd064ec2dc5?mp_source=share)

---

### ğŸ“‹ Project Checklist
- [x] Multi-country data assessment completed
- [x] Column mapping system implemented
- [x] Automated harmonization pipeline built
- [x] Comprehensive validation suite created
- [x] Master dataset successfully generated
- [x] Database hosting solution deployed
- [x] Client training and documentation delivered

**Ready to transform your scattered data into unified insights?** Let's discuss your project requirements.
